{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pyg_lib (from versions: none)\n",
      "ERROR: No matching distribution found for pyg_lib\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torch_geometric\n",
    "\n",
    "# Optional dependencies:\n",
    "!pip install -q pyg_lib torch_scatter torch_sparse torch_cluster \\\n",
    "torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required modules:\n",
    "import scipy.io as sc\n",
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, GraphConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import random\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rewrite the .mat files into a useable format:\n",
    "Then save the data into useable variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: './Documents/Git_coding_respositories/ECE_Project_2/Deep-learn'\n",
      "c:\\Users\\Adam\\Documents\\Git_Coding_respositories\\ECE_Project_2\\Deep-learn\n"
     ]
    }
   ],
   "source": [
    "%cd ./Documents/Git_coding_respositories/ECE_Project_2/Deep-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Adam\\\\Documents\\\\Git_coding_respositories\\\\ECE_Project_2\\\\Deep-learn'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory if it doesn't exist\n",
    "if os.path.exists(\"./data_processed/BP\") == False:\n",
    "    os.makedirs(\"./data_processed/BP\")\n",
    "    os.makedirs(\"./data_processed/NC\")\n",
    "    os.makedirs(\"./data_processed/All\")\n",
    "\n",
    "file_root = \"./data\"\n",
    "file_root_data_path = os.listdir(file_root)\n",
    "bp = []\n",
    "\n",
    "for file in file_root_data_path:\n",
    "    mat1 = sc.loadmat(\"{}/{}\".format(file_root, file))\n",
    "    mat1 = np.array(mat1[\"C\"])\n",
    "    # Process each file type into separate folders in case needed\n",
    "    if re.search(\"BP\", file):\n",
    "        with open(\"./data_processed/BP/{}.txt\".format(os.path.splitext(file)[0]), \"w\") as f:\n",
    "            csv.writer(f, delimiter=',').writerows(mat1)\n",
    "    elif re.search(\"NC\", file):\n",
    "        with open(\"./data_processed/NC/{}.txt\".format(os.path.splitext(file)[0]), \"w\") as f:\n",
    "            csv.writer(f, delimiter=',').writerows(mat1)\n",
    "    # Saved all files into an all folder\n",
    "    with open(\"./data_processed/All/{}.txt\".format(os.path.splitext(file)[0]), \"w\") as f:\n",
    "        csv.writer(f, delimiter=',').writerows(mat1)\n",
    "        \n",
    "# Load the adjacency matrices from the text files as numpy arrays:\n",
    "import pandas as pd\n",
    "\n",
    "bp_root = \"./data_processed/BP\"\n",
    "bp_root_path = os.listdir(bp_root)\n",
    "nc_root = \"./data_processed/NC\"\n",
    "nc_root_path = os.listdir(nc_root)\n",
    "all_files_root = \"./data_processed/All\"\n",
    "all_files_root_path = os.listdir(all_files_root)\n",
    "bp = []\n",
    "nc = []\n",
    "all_files = []\n",
    "\n",
    "for file in bp_root_path:\n",
    "    mat1 = pd.read_csv(\"{}/{}\".format(bp_root,file), header=None).to_numpy()\n",
    "    bp.append(mat1)\n",
    "\n",
    "for file in nc_root_path:\n",
    "    mat1 = pd.read_csv(\"{}/{}\".format(nc_root,file), header=None).to_numpy()\n",
    "    nc.append(mat1)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for file in all_files_root_path:\n",
    "    mat1 = pd.read_csv(\"{}/{}\".format(all_files_root,file), header=None).to_numpy()\n",
    "    if re.search(\"BP\", file):\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "    all_files.append(mat1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Setup:\n",
    "Rewrite .mat files into a usable format, Load those files into variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Adam\\anaconda3\\Lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# Convert the adjacency matrices to dataframes that can be used in pytorch:\n",
    "# Function to calculate the edge_matrix\n",
    "def calc_edge_matrix(adjacency_matrices):\n",
    "        num_nodes = adjacency_matrices[0].shape[0]\n",
    "        edge_index = []\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i + 1, num_nodes):\n",
    "                weight = all_files[i][j]\n",
    "                if weight.any != 0:\n",
    "                    edge_index.append([i, j])\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        return edge_index\n",
    "\n",
    "# Prepare Data for PyTorch\n",
    "def prepare_data(adjacency_matrices, labels):\n",
    "    graph_data = []\n",
    "    for adj_matrix, label in zip(adjacency_matrices, labels):\n",
    "        edge_index = calc_edge_matrix(adjacency_matrices)\n",
    "        x = torch.tensor((adj_matrix), dtype=torch.float)  # Features (e.g., all ones)\n",
    "        y = torch.tensor([label], dtype=torch.long)\n",
    "        num_classes = 2\n",
    "        num_features = len(bp[0])\n",
    "        num_nodes = len(bp[0])\n",
    "        num_edges = 3486\n",
    "        data = Data(x=x, edge_index=edge_index, y=y, num_classes=num_classes, num_features = num_features, num_nodes = num_nodes, num_edges = num_edges)\n",
    "        graph_data.append(data)\n",
    "    return graph_data\n",
    "# Create a dataset from the prepared data\n",
    "graph_data = prepare_data(all_files, labels)\n",
    "\n",
    "# Perform the K-fold cross-validation splits:\n",
    "k = 5 # number of spits to perform, must be 5 for 80/20\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=499)\n",
    "random.seed(412)\n",
    "shuffled_graph_data = random.sample(graph_data, len(graph_data))\n",
    "train_mask = []\n",
    "test_mask = []\n",
    "\n",
    "# Generate the train and test mask for selecting the data:\n",
    "for i, (train_index, test_index) in enumerate(kf.split(shuffled_graph_data)):\n",
    "    train_mask.append(train_index)\n",
    "    test_mask.append(test_index)\n",
    "\n",
    "train_data = [[] for _ in range(5)]\n",
    "for i in range(len(train_mask)):\n",
    "    for j in train_mask[i]:\n",
    "        train_data[i].append(shuffled_graph_data[j])\n",
    "\n",
    "test_data = [[] for _ in range(5)]\n",
    "for i in range(len(test_mask)):\n",
    "    for j in test_mask[i]:\n",
    "        test_data[i].append(shuffled_graph_data[j])\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=7, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Data(x=[84, 84], edge_index=[2, 3486], y=[1], num_classes=2, num_features=84, num_nodes=84, num_edges=3486):\n",
      "====================\n",
      "Number of graphs: 97\n",
      "Number of features: 84\n",
      "Number of classes: 2\n",
      "\n",
      "Data(x=[84, 84], edge_index=[2, 3486], y=[1], num_classes=2, num_features=84, num_nodes=84, num_edges=3486)\n",
      "=============================================================\n",
      "Number of nodes: 84\n",
      "Number of edges: 3486\n",
      "Average node degree: 41.50\n"
     ]
    }
   ],
   "source": [
    "# Grab information about the dataset:\n",
    "print()\n",
    "print(f'Dataset: {graph_data[0]}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(graph_data)}')\n",
    "print(f'Number of features: {graph_data[0].num_features}')\n",
    "print(f'Number of classes: {graph_data[0].num_classes}')\n",
    "\n",
    "data = graph_data[0]\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Instantiate the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv1d, BatchNorm1d, MaxPool1d, Softmax2d, Conv3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): Conv1d(84, 64, kernel_size=(1,), stride=(1,))\n",
      "  (conv2): BatchNorm1d(84, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): MaxPool1d(kernel_size=64, stride=64, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  (conv5): BatchNorm1d(64, eps=1, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv6): MaxPool1d(kernel_size=64, stride=64, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv7): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  (conv8): BatchNorm1d(64, eps=64, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv9): MaxPool1d(kernel_size=64, stride=64, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (sm): Softmax2d()\n",
      "  (lin): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, output_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(23145)\n",
    "        self.conv1 = Conv1d(84, hidden_channels, 1)\n",
    "        self.conv2 = BatchNorm1d(84, hidden_channels)\n",
    "        self.conv3 = MaxPool1d(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.conv4 = Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv5 = BatchNorm1d(hidden_channels, 1)\n",
    "        self.conv6 = MaxPool1d(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.conv7 = Conv1d(hidden_channels, hidden_channels, 1)\n",
    "        self.conv8 = BatchNorm1d(hidden_channels, hidden_channels)\n",
    "        self.conv9 = MaxPool1d(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Create classification\n",
    "        self.fc1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.fc2 = Linear(hidden_channels, output_channels)\n",
    "        self.sm = Softmax2d()\n",
    "        self.lin = Linear(output_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv9(x)\n",
    "\n",
    "        # Create classification head:\n",
    "        x = self.fc1(x)\n",
    "        x = x.relu()\n",
    "        x = self.fc2(x)\n",
    "        x = self.sm(x)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64, output_channels = 64)\n",
    "print(model)\n",
    "\n",
    "# Define train and test functions\n",
    "def train(loader, k):\n",
    "    model.train()\n",
    "\n",
    "    for data in loader.dataset[k]:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader, k):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader.dataset[k]:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset[k]) # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and then evaluate the GNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== K-fold data split 1 ====================\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 1 elements not 64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Adam\\Documents\\Git_Coding_respositories\\ECE_Project_2\\Deep-learn\\Project_2_with_K_cluster.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m==================== K-fold data split \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m ====================\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     train(train_loader, i)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     train_acc \u001b[39m=\u001b[39m test(train_loader, i)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     train_acc_history\u001b[39m.\u001b[39mappend(train_acc)\n",
      "\u001b[1;32mc:\\Users\\Adam\\Documents\\Git_Coding_respositories\\ECE_Project_2\\Deep-learn\\Project_2_with_K_cluster.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m loader\u001b[39m.\u001b[39mdataset[k]:  \u001b[39m# Iterate in batches over the training dataset.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index)  \u001b[39m# Perform a single forward pass.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(out, data\u001b[39m.\u001b[39my)  \u001b[39m# Compute the loss.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Adam\\Documents\\Git_Coding_respositories\\ECE_Project_2\\Deep-learn\\Project_2_with_K_cluster.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(x)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv5(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrelu()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Adam/Documents/Git_Coding_respositories/ECE_Project_2/Deep-learn/Project_2_with_K_cluster.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv6(x)\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrack_running_stats \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps,\n\u001b[0;32m    183\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Adam\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2476\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2478\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbatch_norm(\n\u001b[0;32m   2479\u001b[0m     \u001b[39minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39mbackends\u001b[39m.\u001b[39mcudnn\u001b[39m.\u001b[39menabled\n\u001b[0;32m   2480\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 1 elements not 64"
     ]
    }
   ],
   "source": [
    "master_train_acc = []\n",
    "master_test_acc = []\n",
    "\n",
    "#=========== Loop variables ========================\n",
    "counter = k # Number of k-fold splits being performed\n",
    "num_epochs = 35\n",
    "\n",
    "# Model variables\n",
    "hidden_channels = 64\n",
    "output_channels = 64\n",
    "#===================================================\n",
    "for i in range(counter):\n",
    "    train_acc_history = []\n",
    "    test_acc_history = []\n",
    "    model = GCN(hidden_channels, output_channels)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    if i == 0:\n",
    "        print(f\"==================== K-fold data split {i+1} ====================\")\n",
    "    else:\n",
    "        print(f\"\\n\\n==================== K-fold data split {i+1} ====================\")\n",
    "    for epoch in range(num_epochs+1):\n",
    "        train(train_loader, i)\n",
    "        train_acc = test(train_loader, i)\n",
    "        train_acc_history.append(train_acc)\n",
    "        test_acc = test(test_loader, i)\n",
    "        test_acc_history.append(test_acc)\n",
    "        print(f'Epoch: {epoch:03d}, Train Acc: {100*train_acc:.4f}%, Test Acc: {100*test_acc:.4f}%')\n",
    "\n",
    "    master_train_acc.append(train_acc)\n",
    "    master_test_acc.append(test_acc)\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.title(f\"K-fold data split {i+1}\")\n",
    "    plt.plot(range(1, num_epochs+2), train_acc_history, label='Training Accuracy')\n",
    "    plt.plot(range(1, num_epochs+2), test_acc_history, label='Testing Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"The overall average training accuracy is: {np.mean(master_train_acc):.4f}\")\n",
    "print(f\"The overall average testing accuracy is: {np.mean(master_test_acc):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Light probability analysis to determine if the model was just getting lucky:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic probability analysis: Is the model simply getting lucky with random guesses?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For dataset 1:\n",
      "The random chance that the model predicts the ones class: 0.3\n",
      "The random chance that the model precicts the zeros class: 0.7\n",
      "============================================================\n",
      "For dataset 2:\n",
      "The random chance that the model predicts the ones class: 0.425\n",
      "The random chance that the model precicts the zeros class: 0.575\n",
      "============================================================\n",
      "For dataset 3:\n",
      "The random chance that the model predicts the ones class: 0.5254237288135594\n",
      "The random chance that the model precicts the zeros class: 0.4745762711864407\n",
      "============================================================\n",
      "For dataset 4:\n",
      "The random chance that the model predicts the ones class: 0.5512820512820513\n",
      "The random chance that the model precicts the zeros class: 0.44871794871794873\n",
      "============================================================\n",
      "For dataset 5:\n",
      "The random chance that the model predicts the ones class: 0.5360824742268041\n",
      "The random chance that the model precicts the zeros class: 0.4639175257731959\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Probability analysis -- are my results entirely from chance?\n",
    "y = []\n",
    "ones = 0\n",
    "zeros = 0\n",
    "for i in range(len(test_data)):\n",
    "        for item in test_data[i]:\n",
    "                y.append(item.y.tolist())\n",
    "                if item.y == 1:\n",
    "                        ones += 1\n",
    "                else:\n",
    "                        zeros += 1\n",
    "        print(f\"For dataset {i+1}:\")\n",
    "        print(f\"The random chance that the model predicts the ones class: {ones / len(y)}\")\n",
    "        print(f\"The random chance that the model precicts the zeros class: {zeros/ len(y)}\")\n",
    "        print(\"============================================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
